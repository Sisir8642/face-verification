{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f69859e1-7d11-44a3-b666-fda15050178d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/baburambista/Desktop/face-verification/face-datasets'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "828e1857-9d74-4cf8-b456-b2f46be3462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "IMG_SIZE = 160\n",
    "ROOT = \"/Users/baburambista/Desktop/face-verification/face-datasets/\"\n",
    "\n",
    "df = pd.read_csv(ROOT + \"face_identification.csv\")\n",
    "\n",
    "def load_image(path):\n",
    "    # path = ROOT + path\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    return img / 255.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c862d4f-8e30-4483-a755-8247e8e172d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(df): #loads image row by row\n",
    "    for _, row in df.iterrows():       #_ is the throwuse field(index, row) as iterrow gives us that two values\n",
    "        img1 = load_image(ROOT + row[\"image_1\"]) #It looks up the value for the column \"image_2\".\n",
    "        img2 = load_image(ROOT + row[\"image_2\"])\n",
    "        label = float(row[\"target\"])\n",
    "        yield (img1, img2), label #The generator produces one element at a time.Each element is a tuple of two things: img and label\n",
    "\n",
    "##creates the dataset that Batches them into groups here 8, images at a time Prefetches them for efficient GPU/CPU training\n",
    "dataset = tf.data.Dataset.from_generator( #tf.data like a folder of tools for loading, transforming, and feeding data efficiently. Dataset is a class inside tf.data from which we can iterate over, batch, shuffle, and feed to a model.\n",
    "    lambda: data_generator(df), #lamba returns no arguments \n",
    "    output_signature=( #defines the shape , type of the data\n",
    "        (\n",
    "            tf.TensorSpec((IMG_SIZE,IMG_SIZE,3), tf.float32), #yield value is used here ((img1_tensor, img2_tensor), 1)\n",
    "            tf.TensorSpec((IMG_SIZE,IMG_SIZE,3), tf.float32),\n",
    "        ),\n",
    "        tf.TensorSpec((), tf.int32)\n",
    "    )\n",
    ")\n",
    "\n",
    "dataset = dataset.shuffle(1024).batch(8).repeat().prefetch(tf.data.AUTOTUNE) #we combine the 8 row in one for training (8, 412, 412, 3)\n",
    "\n",
    "steps_per_epoch = len(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bcd4ec52-97f8-4222-bff9-0b7f7d2feb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually trained model\n",
    "# from tensorflow.keras import layers, Model\n",
    "\n",
    "# def build_encoder():\n",
    "#     inp = layers.Input((IMG_SIZE,IMG_SIZE,3))\n",
    "\n",
    "#     x = layers.Conv2D(32, 3, activation='relu')(inp)\n",
    "#     x = layers.MaxPool2D()(x)\n",
    "\n",
    "#     x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "#     x = layers.MaxPool2D()(x)\n",
    "\n",
    "#     x = layers.Conv2D(128, 3, activation='relu')(x)\n",
    "#     x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "#     x = layers.Dense(128)(x)  # embedding vector\n",
    "#     x = layers.Lambda(lambda t: tf.math.l2_normalize(t, axis=1))(x)\n",
    "\n",
    "#     return Model(inp, x)\n",
    "\n",
    "# encoder = build_encoder()\n",
    "# encoder.summary()\n",
    "\n",
    "#using the pretraied model \n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "def build_encoder():\n",
    "    inp = layers.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    \n",
    "    # Pretrained MobileNetV2, exclude top layers\n",
    "    base_model = MobileNetV2(input_tensor=inp, include_top=False, weights='imagenet')\n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128)(x)  # embedding vector\n",
    "    x = layers.Lambda(lambda t: tf.math.l2_normalize(t, axis=1))(x)  # L2 normalize\n",
    "\n",
    "    return Model(inp, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51c29150-d37c-45be-8989-1e89fc30bc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 64ms/step - loss: 0.1118    \n",
      "Epoch 2/5\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 67ms/step - loss: 0.1372 \n",
      "Epoch 3/5\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 70ms/step - loss: 0.1327 \n",
      "Epoch 4/5\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 71ms/step - loss: 0.1249 \n",
      "Epoch 5/5\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 76ms/step - loss: 0.1182 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x11ea1fad0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1 = layers.Input((IMG_SIZE,IMG_SIZE,3))\n",
    "img2 = layers.Input((IMG_SIZE,IMG_SIZE,3))\n",
    "\n",
    "e1 = encoder(img1)\n",
    "e2 = encoder(img2)\n",
    "\n",
    "# Euclidean distance\n",
    "distance = layers.Lambda(lambda tensors: tf.norm(tensors[0] - tensors[1], axis=1, keepdims=True))([e1, e2])\n",
    "model = Model([img1, img2], distance)\n",
    "\n",
    "def contrastive_loss(y_true, d, margin=1):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    return tf.reduce_mean(\n",
    "        y_true * tf.square(d) + \n",
    "        (1 - y_true) * tf.square(tf.maximum(margin - d, 0))\n",
    "    )\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss=lambda y, d: contrastive_loss(y, d, margin=1)  # margin=1 is standard\n",
    ")\n",
    "\n",
    "steps_per_epoch = len(df) // 8\n",
    "\n",
    "model.fit(dataset, epochs=5, steps_per_epoch=steps_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1045c36-64b1-41b9-9ed1-3c70d0243efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66b3bb45-7146-4051-9149-5de77ef69d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Distance: 1.3585352\n",
      "Same person? False\n"
     ]
    }
   ],
   "source": [
    "img1 = load_image(ROOT + \"faces/Alexis Bledel/cropped_Alexis Bledel_4.jpg\")\n",
    "img2 = load_image(ROOT + \"faces/Alexis Bledel/cropped_Alexis Bledel_10.jpg\")\n",
    "\n",
    "img1 = tf.expand_dims(img1, axis=0)\n",
    "img2 = tf.expand_dims(img2, axis=0)\n",
    "\n",
    "dist = model.predict([img1, img2])[0][0]\n",
    "print(\"Distance:\", dist)\n",
    "\n",
    "\n",
    "# ✅ Threshold check\n",
    "THRESHOLD = 0.5\n",
    "same = dist < THRESHOLD\n",
    "print(\"Same person?\", same)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1449c62c-2c85-4845-b8db-0f9e691d78b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Distance: 1.318017\n",
      "Same person? False\n"
     ]
    }
   ],
   "source": [
    "img1 = load_image(ROOT + \"faces/Alexis Bledel/cropped_Alexis Bledel_4.jpg\")\n",
    "img2 = load_image(ROOT + \"faces/Adam Sandler/cropped_Adam Sandler_8.jpg\")\n",
    "\n",
    "img1 = tf.expand_dims(img1, axis=0)  # (1, 160, 160, 3)\n",
    "img2 = tf.expand_dims(img2, axis=0)\n",
    "\n",
    "dist = model.predict([img1, img2])[0][0]\n",
    "print(\"Distance:\", dist)\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "same = dist < THRESHOLD\n",
    "print(\"Same person?\", same)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2fc52b-2e33-4a29-9963-4eaa57fc3d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d37c631-7b4c-4d10-8027-b692bb029870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92669eab-cbf5-4fd6-9e64-fa295cf44e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-env)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
